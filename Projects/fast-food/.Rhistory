corpus.n = tm_map(corpus.n, content_transformer(removeURL))#
corpus.n = tm_map(corpus.n, content_transformer(removeHashtags))#
corpus.n = tm_map(corpus.n, content_transformer(removeTwitterHandles))#
corpus.n = tm_map(corpus.n, removeWords, stopwords("english")) #
corpus.n = tm_map(corpus.n, removePunctuation)
corpus.n = tm_map(corpus.n, stemDocument)
# Convert back from corpus object to character vector
neutral.text <- concatenate(lapply(corpus.n, "[", 1))
neutral.ng <- ngram(neutral.text, n=3)
get.phrasetable(neutral.ng)
#Saving this for convenience. Not part of story.
#saveRDS(file="tweets_with_sentiment.rds", tweets)
#saveRDS(file="neg_ng.rds", neg.ng)
#saveRDS(file="pos_ng.rds", pos.ng)
saveRDS(file="neutral_ng.rds", neutral.ng)
# Read the json file using a stream,
# since it is too big to read all at once
tweets <- stream_in(file("tweets.json"), pagesize = 10000, verbose = FALSE)
# Convert Tweet text to lower case, for standardization
tweets$Text <- tolower(tweets$Text)
# Remove duplicates (Tweets that we received from both API calls)
# This does not remove Retweets
tweets <- unique(tweets)
# Identify Tweets using the different hashtags
tweets$DisneyPlus = ifelse(grepl("#disneyplus(?!f)", tweets$Text, perl=TRUE), 1, 0)
tweets$DisneyPlusFail = ifelse(grepl("#disneyplusfail", tweets$Text), 1, 0)
tweets$Both = ifelse(tweets$DisneyPlus == 0, 0, ifelse(tweets$DisneyPlusFail == 1, 1, 0))
# Count the number of Tweets per hashtag on different days
compressed <- tweets %>%
group_by(Created_at) %>%
summarise(DisneyPlus = sum(DisneyPlus),
DisneyPlusFail = sum(DisneyPlusFail),
Both = sum(Both))
compressed$Day <- c("Tuesday - Release Day", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")
# Display results
compressed
# Read the json file using a stream,
# since it is too big to read all at once
tweets <- stream_in(file("tweets.json"), pagesize = 10000, verbose = FALSE)
library(jsonlite)
library(ggplot2)
library(dplyr)
library(tidyr)
library(scales)
library(lubridate)
library(tm)
library(SnowballC)
library(SentimentAnalysis)
library(gridExtra)
library(ngram)
setwd("~/Desktop/Data-Science-Portfolio/Projects/DisneyPlus")
# Read the json file using a stream,
# since it is too big to read all at once
tweets <- stream_in(file("tweets.json"), pagesize = 10000, verbose = FALSE)
# Convert Tweet text to lower case, for standardization
tweets$Text <- tolower(tweets$Text)
# Remove duplicates (Tweets that we received from both API calls)
# This does not remove Retweets
tweets <- unique(tweets)
# Identify Tweets using the different hashtags
tweets$DisneyPlus = ifelse(grepl("#disneyplus(?!f)", tweets$Text, perl=TRUE), 1, 0)
tweets$DisneyPlusFail = ifelse(grepl("#disneyplusfail", tweets$Text), 1, 0)
tweets$Both = ifelse(tweets$DisneyPlus == 0, 0, ifelse(tweets$DisneyPlusFail == 1, 1, 0))
# Count the number of Tweets per hashtag on different days
compressed <- tweets %>%
group_by(Created_at) %>%
summarise(DisneyPlus = sum(DisneyPlus),
DisneyPlusFail = sum(DisneyPlusFail),
Both = sum(Both))
compressed$Day <- c("Tuesday - Release Day", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")
# Display results
compressed
# Categorize Tweets as either having "#DisneyPlus#, "#DisneyPlusFail", or both of them
tweets$Descriptor <- ifelse((tweets$DisneyPlus == 1 & tweets$DisneyPlusFail == 0), "#DisneyPlus",
ifelse((tweets$DisneyPlus == 0 & tweets$DisneyPlusFail == 1),
"#DisneyPlusFail","Both"))
# Create boxplots for our 3 categories of Tweets, and their respective sentiments
p1 <- ggplot(tweets, aes(x=Descriptor, y=SentimentQDAP, color=Descriptor)) +
geom_boxplot() +
theme_minimal() +
labs(x="Hashtag used", y="Sentiment Score") +
theme(legend.position="none") +
ggtitle("Sentiment by Hashtag")
p2 <- ggplot(tweets, aes(y=SentimentQDAP)) +
geom_boxplot() +
theme_minimal() +
ggtitle("Overall Sentiment")
grid.arrange(p1,p2, ncol=2)
#Saving this for convenience. Not part of story.
#saveRDS(file="tweets_with_sentiment.rds", tweets)
#saveRDS(file="neg_ng.rds", neg.ng)
#saveRDS(file="pos_ng.rds", pos.ng)
#saveRDS(file="neutral_ng.rds", neutral.ng)
tweets <- readRDS("tweets_with_sentiment.rds")
# Categorize Tweets as either having "#DisneyPlus#, "#DisneyPlusFail", or both of them
tweets$Descriptor <- ifelse((tweets$DisneyPlus == 1 & tweets$DisneyPlusFail == 0), "#DisneyPlus",
ifelse((tweets$DisneyPlus == 0 & tweets$DisneyPlusFail == 1),
"#DisneyPlusFail","Both"))
# Create boxplots for our 3 categories of Tweets, and their respective sentiments
p1 <- ggplot(tweets, aes(x=Descriptor, y=SentimentQDAP, color=Descriptor)) +
geom_boxplot() +
theme_minimal() +
labs(x="Hashtag used", y="Sentiment Score") +
theme(legend.position="none") +
ggtitle("Sentiment by Hashtag")
p2 <- ggplot(tweets, aes(y=SentimentQDAP)) +
geom_boxplot() +
theme_minimal() +
ggtitle("Overall Sentiment")
grid.arrange(p1,p2, ncol=2)
set.seed(42)
continue.watch <- tweets[which(grepl("continue watching", tweets$Text)),]
examples <- sample_n(continue.watch, 5)
message(paste0("Number of Tweets with the phrase 'continue watching': ", length(rownames((continue.watch)))))
for (i in 1:5) {
message(paste0("Score: ", paste(examples[i,'SentimentQDAP'],
examples[i,'Text'], sep=":")))
}
# Helper function
clean <- function(tweet.subset) {
# Subset so that we are looking at original Tweets only, rather than including Retweets
# Also turns our there are some accounts spamming advertisements for other products,
# using the #DisneyPlus hashtag for publicity. Remove those Tweets + all Retweets,
# since having the exact same Tweets repeatedly will bias our word counts.
tweet.subset <- subset(tweet.subset, tweet.subset$Retweet == "Original Tweet")
tweet.subset <- tweet.subset[!grepl("fortnite|minecraft", tweet.subset$Text),]
tweet.subset <- unique(tweet.subset[,'Text'])
# Merge the negative Tweets into a single character vector
negative.text <- concatenate(tweet.subset)
#text <- concatenate(tweets[,"Text"])
# Perform some cleaning of the text, similar to what was
# done during the sentiment analysis.
corpus = SimpleCorpus(VectorSource(negative.text))
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, content_transformer(removeURL))
corpus = tm_map(corpus, content_transformer(removeHashtags))
corpus = tm_map(corpus, content_transformer(removeTwitterHandles))
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, stemDocument)
corpus
}
# Subset our Tweets to only have the ones with negative sentiment scores
negative.tweets <- subset(tweets, tweets$SentimentQDAP < 0)
# Clean and tokenize Tweets
corpus.neg <- clean(negative.tweets)
removeURL <- function(x) gsub("http[\\S]+[[:space:]]", "", x)
removeHashtags <- function(x) gsub("#\\S+", "", x)
removeTwitterHandles <- function(x) gsub("@\\S+", "", x)
# Subset our Tweets to only have the ones with negative sentiment scores
negative.tweets <- subset(tweets, tweets$SentimentQDAP < 0)
# Clean and tokenize Tweets
corpus.neg <- clean(negative.tweets)
# Convert back from corpus object to character vector
negative.text <- concatenate(lapply(corpus.neg, "[", 1))
neg.ng <- ngram(negative.text, n=3)
get.phrasetable(neg.ng)
# Subset our Tweets to only have the ones with positive sentiment scores
positive.tweets <- subset(tweets, tweets$SentimentQDAP > 0)
corpus.pos <- clean(positive.tweets)
# Convert back from corpus object to character vector
positive.text <- concatenate(lapply(corpus.pos, "[", 1))
pos.ng <- ngram(positive.text, n=3)
get.phrasetable(pos.ng)
setwd("~/Desktop/Data-Science-Portfolio/Projects/fast-food")
# A sample of 10,000 fast food restaurants in the US
# Remember that its a sample of a much larger dataset,
# so any findings here are reflective only of this
# dataset, and may potentially be different than reality.
data <- read.csv("datasets/FastFoodRestaurants.csv")
population <- read.csv("datasets/StatePop.csv")
names(population) <- c("FullName", "state", "x2019PopEst", "x2010Pop")
population$x2010Pop <- as.character(population$x2010Pop)
population$x2010Pop <- gsub(",", "", population$x2010Pop)
population$x2010Pop <- as.numeric(population$x2010Pop)
population$pt <- (population$x2010Pop / sum(population$x2010Pop)) * 100
obesity <- read.csv("datasets/StateObesity.csv")
names(obesity) <- c("FullName", "state", "PercentObese")
data.proc <- data
data.proc$count <- 1
# There are multiple restaurants that appear multiple times, with variations in their names. Fix it
data.proc$name <- tolower(data.proc$name)
data.proc$name <- gsub('[[:punct:]]', '', data.proc$name)
n <- length(unique(data$name)) - length(unique(data.proc$name))
# 61 duplicated names were merged by removing variations in capitilization and punctuation
# Some duplication still exists due to spacing, for example,
# aw all american food and aw allamerican food still exist
data.proc$name <- gsub('[[:space:]]', '', data.proc$name)
p <- length(unique(data$name)) - length(unique(data.proc$name))
# merged an additional 11 restaurants
# There are several more restaurants that have a couple rare variations
# of there names, but because they are rare, they likely won't impact
# analysis much. If a particular restaurant becomes particularly interesting
# look deeper into alternate names for them specifically
# For some reasons Colorado Springs is listed as its own state
data.proc$province <- gsub('Co Spgs', 'CO', data.proc$province)
compressed.name <- data.proc %>%
group_by(name) %>%
summarise(Count = sum(count))
compressed.name <- compressed.name[order(-compressed.name$Count),]
top10 <- compressed.name[1:10,]
sum(top10$Count) # Over 71% of our dataset is made up of 10 restaurants
top10$name <- factor(top10$name, levels=c("mcdonalds", "burgerking", "tacobell",
"wendys", "arbys", "kfc", "subway",
"sonicdrivein", "dominospizza", "jackinthebox"))
library(dplyr)
library(tidyr)
library(ggplot2)
library(usmap)
library(caret)
plot_usmap(data=compressed.state, values = 'pt', color='orange') +
scale_fill_continuous(low="white", high="blue", name="% of Fast Food Restaurants", label=scales::comma) +
theme(legend.position = "none")
compressed.state <- data.proc %>%
group_by(province) %>%
summarise(Count = sum(count))
names(compressed.state) <- c("state", "Count")
compressed.state$pt <- (compressed.state$Count / 10000) * 100
plot_usmap(data=compressed.state, values = 'pt', color='orange') +
scale_fill_continuous(low="white", high="blue", name="% of Fast Food Restaurants", label=scales::comma) +
theme(legend.position = "none")
plot_usmap(data=compressed.state, values = 'pt', color='orange') +
scale_fill_continuous(low="white", high="orange", name="% of Fast Food Restaurants", label=scales::comma) +
theme(legend.position = "none")
figpath <- "figures/foodperstate.jpg"
p <- plot_usmap(data=compressed.state, values = 'pt', color='orange') +
scale_fill_continuous(low="white", high="orange", name="% of Fast Food Restaurants", label=scales::comma) +
theme(legend.position = "none")
ggsave(figpath, p)
figpath <- "foodperstate.jpg"
p <- plot_usmap(data=compressed.state, values = 'pt', color='orange') +
scale_fill_continuous(low="white", high="orange", name="% of Fast Food Restaurants", label=scales::comma) +
theme(legend.position = "none")
ggsave(figpath, p)
figpath <- "foodperstate.jpg"
p <- plot_usmap(data=compressed.state, values = 'pt', color='blue') +
scale_fill_continuous(low="white", high="blue", name="% of Fast Food Restaurants", label=scales::comma) +
theme(legend.position = "none")
ggsave(figpath, p)
rm(list=ls())
clean <- function(tweet.subset) {
# Subset so that we are looking at original Tweets only, rather than including Retweets
# Also turns our there are some accounts spamming advertisements for other products,
# using the #DisneyPlus hashtag for publicity. Remove those Tweets + all Retweets,
# since having the exact same Tweets repeatedly will bias our word counts.
tweet.subset <- subset(tweet.subset, tweet.subset$Retweet == "Original Tweet")
tweet.subset <- tweet.subset[!grepl("fortnite|minecraft", tweet.subset$Text),]
tweet.subset <- unique(tweet.subset[,'Text'])
# Merge the negative Tweets into a single character vector
negative.text <- concatenate(tweet.subset)
#text <- concatenate(tweets[,"Text"])
# Perform some cleaning of the text, similar to what was
# done during the sentiment analysis.
corpus = SimpleCorpus(VectorSource(negative.text))
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, content_transformer(removeURL))
corpus = tm_map(corpus, content_transformer(removeHashtags))
corpus = tm_map(corpus, content_transformer(removeTwitterHandles))
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, stemDocument)
corpus
}
#Saving this for convenience. Not part of story.
#saveRDS(file="tweets_with_sentiment.rds", tweets)
#saveRDS(file="neg_ng.rds", neg.ng)
#saveRDS(file="pos_ng.rds", pos.ng)
#saveRDS(file="neutral_ng.rds", neutral.ng)
tweets <- readRDS("tweets_with_sentiment.rds")
sub <- sample_n(tweets, 25000)
corpus <-  clean(sub)
removeURL <- function(x) gsub("http[\\S]+[[:space:]]", "", x)
removeHashtags <- function(x) gsub("#\\S+", "", x)
removeTwitterHandles <- function(x) gsub("@\\S+", "", x)
clean <- function(tweet.subset) {
# Subset so that we are looking at original Tweets only, rather than including Retweets
# Also turns our there are some accounts spamming advertisements for other products,
# using the #DisneyPlus hashtag for publicity. Remove those Tweets + all Retweets,
# since having the exact same Tweets repeatedly will bias our word counts.
tweet.subset <- subset(tweet.subset, tweet.subset$Retweet == "Original Tweet")
tweet.subset <- tweet.subset[!grepl("fortnite|minecraft", tweet.subset$Text),]
tweet.subset <- unique(tweet.subset[,'Text'])
# Merge the negative Tweets into a single character vector
negative.text <- concatenate(tweet.subset)
#text <- concatenate(tweets[,"Text"])
# Perform some cleaning of the text, similar to what was
# done during the sentiment analysis.
corpus = SimpleCorpus(VectorSource(negative.text))
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, content_transformer(removeURL))
corpus = tm_map(corpus, content_transformer(removeHashtags))
corpus = tm_map(corpus, content_transformer(removeTwitterHandles))
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, removePunctuation)
#corpus = tm_map(corpus, stemDocument)
corpus
}
sub <- sample_n(tweets, 25000)
corpus <-  clean(sub)
dtm <- TermDocumentMatrix(corpus)
m <- as.matrix(dtm)
v <- sort(rowSums(m), decreasing=TRUE)
d <- data.frame(word=names(v), freq=v)
wordcloud(words = )
View(d)
d <- subset(d, length(d$word) >= 3)
View(d)
length(d#word) >= 3
)
length(d$word) >= 3
words <- d$word
words >= 3
nchar(d$word)
nchar(words)
d.three <- d[nchar(d$word) >= 3,]
l = c()
for (word in words) {
l = c(l, length(word) > 3)
}
dsub <- d[l,]
for (word in words) {
word
}
for (word in words) {
print(word)
}
for (word in words) {
print(length(word))
}
for (word in words) {
print(nchar(word))
}
l = c()
for (word in words) {
l = c(l, nchar(word))
}
dsub <- d[l,]
View(dsub)
d <- data.frame(word=names(v), freq=v)
words <- d$word
class(d$word)
d$word <- as.character(d$word)
d <- subset(d, nchar(d$word) > 3)
install.packages("wordcloud")
library(wordcloud)
library(RColorBrewer)
wordcloud(words = d$word, freq= d$freq, min.freq = 5,
max.words = 200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
tweet.subset <- sample_n(tweets, 20000)
tweet.subset <- tweet.subset[!grepl("fortnite|minecraft", tweet.subset$Text),]
tweet.subset <- unique(tweet.subset[,'Text'])
negative.text <- concatenate(tweet.subset)
corpus = SimpleCorpus(VectorSource(negative.text))
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, content_transformer(removeURL))
corpus = tm_map(corpus, content_transformer(removeHashtags))
corpus = tm_map(corpus, content_transformer(removeTwitterHandles))
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, removePunctuation)
dtm <- TermDocumentMatrix(corpus)
mat <- as.matrix(dtm)
vec <- sort(rowSums(m), decreasing=TRUE)
df <- data.frame(word=names(v), freq=v)
df <- subset(df, nchar(df$word) > 3)
d <- data.frame(word=names(v), freq=v)
d <- subset(d, nchar(d$word) > 3)
v <- sort(rowSums(m), decreasing=TRUE)
d <- data.frame(word=names(v), freq=v)
d <- subset(d, nchar(d$word) > 3)
wordcloud(words = df$word, freq= d$freq, min.freq = 5,
max.words = 200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
df <- subset(df, df$word != "'s")
wordcloud(words = df$word, freq= d$freq, min.freq = 5,
max.words = 200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
df <- subset(df, df$word != "'m")
wordcloud(words = df$word, freq= d$freq, min.freq = 5,
max.words = 200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = df$word, freq= d$freq, min.freq = 5,
max.words = 200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Blues"))
df <- subset(df, df$word != "'m")
wordcloud(words = df$word, freq= d$freq, min.freq = 5,
max.words = 200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Blues"))
View(df)
wordcloud(words = df$word, freq= d$freq, min.freq = 5,
max.words = 200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "BrBG"))
wordcloud(words = df$word, freq= d$freq, min.freq = 5,
max.words = 200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "BuGn"))
wordcloud(words = df$word, freq= d$freq, min.freq = 5,
max.words = 200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(9, "Spectral"))
wordcloud(words = df$word, freq= d$freq, min.freq = 5,
max.words = 200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(9, "RdYlBu"))
wordcloud(words = df$word, freq= d$freq, min.freq = 5,
max.words = 200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(9, "Pastel2"))
wordcloud(words = df$word, freq= d$freq, min.freq = 5,
max.words = 200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Pastel2"))
wordcloud(words = df$word, freq= d$freq, min.freq = 5,
max.words = 200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Set1"))
wordcloud(words = df$word, freq= d$freq, min.freq = 5,
max.words = 200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Paired"))
df <- df[!(row.names(df) &in% c("'s", "'m"))]
df <- df[!(row.names(df) %in% c("'s", "'m"))]
df <- df[!(row.names(df) %in% c("'s", "'m")),]
wordcloud(words = df$word, freq= d$freq, min.freq = 5,
max.words = 200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Paired"))
wordcloud(words = df$word, freq= d$freq, min.freq = 5,
max.words = 200, random.order=FALSE, rot.per=0.1,
colors=brewer.pal(8, "Paired"))
set.seed(42)
wordcloud(words = df$word, freq= d$freq, min.freq = 5,
max.words = 200, random.order=FALSE, rot.per=0.1,
colors=brewer.pal(8, "Paired"))
wordcloud(words = d$word, freq= d$freq, min.freq = 5,
max.words = 200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Paired"))
ggsave("wordcloud.jpg", w)
w <- wordcloud(words = d$word, freq= d$freq, min.freq = 5,
max.words = 200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Paired"))
ggsave("wordcloud.jpg", w)
ggsave("~/Desktop/Data-Science-Portfolio/Projects/DisneyPlus/wordcloud.jpg", w)
w <- wordcloud(words = d$word, freq= d$freq, min.freq = 5,
max.words = 200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Paired"))
ggsave("~/Desktop/Data-Science-Portfolio/Projects/DisneyPlus/wordcloud.jpg", w)
library(jsonlite)
library(ggplot2)
library(dplyr)
library(tidyr)
library(scales)
library(lubridate)
library(tm)
library(SnowballC)
library(SentimentAnalysis)
library(gridExtra)
library(ngram)
library(wordcloud)
library(RColorBrewer)
setwd("~/Desktop/Data-Science-Portfolio/Projects/DisneyPlus")
# Read the json file using a stream,
# since it is too big to read all at once
tweets <- stream_in(file("tweets.json"), pagesize = 10000, verbose = FALSE)
# Convert Tweet text to lower case, for standardization
tweets$Text <- tolower(tweets$Text)
# Remove duplicates (Tweets that we received from both API calls)
# This does not remove Retweets
tweets <- unique(tweets)
# Identify Tweets using the different hashtags
tweets$DisneyPlus = ifelse(grepl("#disneyplus(?!f)", tweets$Text, perl=TRUE), 1, 0)
tweets$DisneyPlusFail = ifelse(grepl("#disneyplusfail", tweets$Text), 1, 0)
tweets$Both = ifelse(tweets$DisneyPlus == 0, 0, ifelse(tweets$DisneyPlusFail == 1, 1, 0))
# Count the number of Tweets per hashtag on different days
compressed <- tweets %>%
group_by(Created_at) %>%
summarise(DisneyPlus = sum(DisneyPlus),
DisneyPlusFail = sum(DisneyPlusFail),
Both = sum(Both))
compressed$Day <- c("Tuesday - Release Day", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")
# Display results
compressed
#Saving this for convenience. Not part of story.
#saveRDS(file="tweets_with_sentiment.rds", tweets)
#saveRDS(file="neg_ng.rds", neg.ng)
#saveRDS(file="pos_ng.rds", pos.ng)
#saveRDS(file="neutral_ng.rds", neutral.ng)
tweets <- readRDS("tweets_with_sentiment.rds")
# Categorize Tweets as either having "#DisneyPlus#, "#DisneyPlusFail", or both of them
tweets$Descriptor <- ifelse((tweets$DisneyPlus == 1 & tweets$DisneyPlusFail == 0), "#DisneyPlus",
ifelse((tweets$DisneyPlus == 0 & tweets$DisneyPlusFail == 1),
"#DisneyPlusFail","Both"))
# Create boxplots for our 3 categories of Tweets, and their respective sentiments
p1 <- ggplot(tweets, aes(x=Descriptor, y=SentimentQDAP, color=Descriptor)) +
geom_boxplot() +
theme_minimal() +
labs(x="Hashtag used", y="Sentiment Score") +
theme(legend.position="none") +
ggtitle("Sentiment by Hashtag")
p2 <- ggplot(tweets, aes(y=SentimentQDAP)) +
geom_boxplot() +
theme_minimal() +
ggtitle("Overall Sentiment")
grid.arrange(p1,p2, ncol=2)
set.seed(42)
continue.watch <- tweets[which(grepl("continue watching", tweets$Text)),]
examples <- sample_n(continue.watch, 5)
message(paste0("Number of Tweets with the phrase 'continue watching': ", length(rownames((continue.watch)))))
for (i in 1:5) {
message(paste0("Score: ", paste(examples[i,'SentimentQDAP'],
examples[i,'Text'], sep=":")))
}
