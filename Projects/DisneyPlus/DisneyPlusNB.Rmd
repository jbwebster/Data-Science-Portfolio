---
title: "Disney+ Opening Week"
subtitle: "Customer Satisfaction and Public Perception on Opening Week"
author: Jace Webster
output: html_notebook
---

```{r, include=FALSE}
library(jsonlite)
library(ggplot2)
library(dplyr)
library(tidyr)
library(scales)
library(lubridate)
library(tm)
library(SnowballC)
library(SentimentAnalysis)
library(gridExtra)
library(ngram)
library(wordcloud)
library(RColorBrewer)
setwd("~/Desktop/Data-Science-Portfolio/Projects/DisneyPlus")
```




## Introduction

Sentiment analysis is a useful branch of natural language processing that allows for the quick analysis of the overall feeling of a piece of text. In practice, that means it is possible to **analyze text to better understand opinions**, and is often used to better understand customers by analyzing things like reviews or survey responses. Here, **we'll be applying those methods to better understand the reaction on Twitter to Disney's streaming service, Disney+, during the days immediately following its release in the United States.** While the streaming service was highly anticipated among Disney fans, there were some immediate technical problems following the release. Knowing that will help guide our analysis. We'll also be doing some other forms of analysis as well.

Disney+ was released in the United States on November 12th, 2019. A few days after the release of the service, I was able to create a Python script using the Tweepy library and Twitter's API to collect data on **over 100,000 Tweets about Disney+ posted in the days immediately following its official release in the US**. A quick look at Twitter (and an official press release from Disney) shows that while many people love the new streaming service, technical issues combined with high expectations led to many people being upset with how the release went. **This project uses 100,000+ Tweets to offer data-supported feedback to Disney+ after its first few days in the US.** 

As mentioned, a Python script (below) was used to retrieve the Tweets from Twitter. Twitter's API has some unfortunate limitations, including the facts that retrieving Tweets by date is essentially impossible, and that when searching by a topic, Tweets older than about a week are inaccessible. So instead, the script was set to retrieve the **100,000 most recent Tweets that included the hashtag #DisneyPlus, as well as the 100,000 most recent Tweets that had the #DisneyPlusFail hashtag** (both trending Tweets from the week of the release). After submitting the calls to the Twitter API (and waiting several hours), the retrieved Tweets were saved for further use.

```{python, eval = FALSE}
# Define a TwitterClient object for using Tweepy and Twitter's API
class TwitterClient(object):
  def __init__(self):
    # Keys and tokens generated by Twitter
    api_key = # Personal api key here
    api_secret = # Personal api secret here
    access_token = # Personal access token here
    access_secret = # Personall access token secret here
    try:
      # Create OAuthHandler object
      self.auth = OAuthHandler(api_key, api_secret)
      # Set token
      self.auth.set_access_token(access_token, access_secret)
      # Create tweepy object
      self.api = tweepy.API(self.auth)
    except:
      print('Error: Authentication failure')
    
  # Get a given number of Tweets using the provided query (#DisneyPlus or #DisneyPlusFail)
  def get_tweets(self, query, count):
    tweets = []
    try:
      # Fetch and store tweets
      for tweet in tweepy.Cursor(self.api.search, q=query, lang='en', tweet_mode = 'extended', wait_on_rate_limit = True).items(count):
        if 'retweeted_status' in dir(tweet):
          full_text = tweet.retweeted_status.full_text
          retweet = "Retweet"
        else:
          full_text = tweet.full_text
          retweet = "Original Tweet"
        dict = {'Screen Name': tweet.user.screen_name,
                'User Name': tweet.user.name,
                'Text': full_text,
                'Retweet': retweet,
                'Created_at': tweet.created_at.strftime("%d-%b-%Y")
        }
        tweets.append(dict)
    except tweepy.TweepError as e:
      print('Error: ' + str(e))
    return tweets

# Create TwitterClient object, as defined above    
api = TwitterClient()

# Attempt to get 100,000 #DisneyPlus Tweets
disney_plus_tweets = api.get_tweets(query = "#DisneyPlus", count = 100000)
# Attempt to get 100,000 #DisneyPlusFail Tweets
disney_plus_fail_tweets = api.get_tweets(query = "#DisneyPlusFail", count = 100000)

# Save tweets to files, so we don't have to use the api anymore
with open('backup_DisneyPlus.json', 'w') as f:
  json.dump(disney_plus_tweets, f)
with open('backup_DisneyPlusFail.json', 'w') as f:
  json.dump(disney_plus_tweets, f)
  
# Merge the tweets and save into one combined file
tweets = []
for tweet in disney_plus_tweets:
  tweets.append(tweet)
for tweet in disney+plus_fail_tweets:
  tweets.append(tweet)
with open('tweets.json', 'w') as f:
  json.dump(tweets)
```




## Analysis and Results
### Data Exploration

Before we worry about what people are saying, let's try to understand the nature of our dataset a bit further. For example, how many posts per day are we looking at? Keep in mind that our API call asked for the last 100,000 Tweets, but that the Twitter API will only get Tweets from the last week. 

```{r, message=FALSE, warning=FALSE,  results = 'hide'}
# Read the json file using a stream,
# since it is too big to read all at once
tweets <- stream_in(file("tweets.json"), pagesize = 10000, verbose = FALSE)
# Convert Tweet text to lower case, for standardization 
tweets$Text <- tolower(tweets$Text)
# Remove duplicates (Tweets that we received from both API calls)
# This does not remove Retweets
tweets <- unique(tweets)
# Identify Tweets using the different hashtags
tweets$DisneyPlus = ifelse(grepl("#disneyplus(?!f)", tweets$Text, perl=TRUE), 1, 0)
tweets$DisneyPlusFail = ifelse(grepl("#disneyplusfail", tweets$Text), 1, 0)
tweets$Both = ifelse(tweets$DisneyPlus == 0, 0, ifelse(tweets$DisneyPlusFail == 1, 1, 0))
# Count the number of Tweets per hashtag on different days
compressed <- tweets %>%
  group_by(Created_at) %>%
  summarise(DisneyPlus = sum(DisneyPlus), 
            DisneyPlusFail = sum(DisneyPlusFail),
            Both = sum(Both))
compressed$Day <- c("Tuesday - Release Day", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")
# Display results
compressed
```

We got `r as.integer(sum(compressed$DisneyPlusFail))` Tweets with the #DisneyPlusFail, which suggests that we got every Tweet with that hashtag during the time range that Twitter has available to us (since we asked for much more than that). We see that **most people using #DisneyPlusFail made those tweets either the day of, or the day after the release, with a small spike of activity on the 16th, which was a Saturday and likely a day when the streaming service was particularly active**. This makes sense, since we would expect that hashtag to be popular at times when most people would first be discovering problems.

We have to be a bit wary with the #DisneyPlus tweets, because we got a total of `r as.integer(sum(compressed$DisneyPlus))` Tweets that contain #DisneyPlus, but we only asked for 100,000. The original API request returned 100,000 #DisneyPlus Tweets to us, but there is some overlap with the #DisneyPlusFail Tweets, so we actually have slightly more than 100,000 for that category. A close look at our backup_DisneyPlus.json file shows that the last Tweet returned from that API call was created on November 13th. That means we hit our 100,000th Tweet sometime on November 13th, and that **all remaining #DisneyPlus Tweets on the 13th and on the 12th are actually Tweets that we received from our #DisneyPlusFail API call**. This is confirmed by the fact that all 807 #DisneyPlus Tweets on the 12th also had #DisneyPlusFail in the Tweet (based on the 'Both' column). Together, this tells us that **rather than peaking on the 14th, #DisneyPlus probably peaked sometime before the 14th, and slowly declined in popularity each day**. It also means we have to be careful about sentiment analysis on the 12th and 13th, since those days have more #DisneyPlusFail Tweets than we may have originally thought. Unfortunately, because the Twitter API only allows you to pull Tweets from within the last few days, and we didn't discover this missing data until a few days after using their API, we are unable to use the API to go back and get the Tweets that we missed.

### Sentiment Analysis 

With that in mind, let's start digging in to what these Tweets are about. I've begun with a standard sentimental analysis using the SentimentAnalysis package, to see if most Tweets are 'positive', 'negative', or 'neutral' in nature. This is done by tokenizing each Tweet (breaking them into individual words) to generate a Bag of Words, and then checking the count/proportion of words in each Tweet that are considered 'positive' or 'negative'. Unfortunately, this is an imperfect system, limited by the words defined in our dictionary and by our method's (in)ability to understand context clues.


```{r}
# Calculate sentiment of each Tweet using the SentimentAnalysis package
removeURL <- function(x) gsub("http[\\S]+[[:space:]]", "", x)
removeHashtags <- function(x) gsub("#\\S+", "", x)
removeTwitterHandles <- function(x) gsub("@\\S+", "", x)

getSentiment <- function(df) {
  # Convert Tweet texts to corpus datatype
  corpus = SimpleCorpus(VectorSource(df$Text))
  # Continue to clean corpus
  corpus = tm_map(corpus, stripWhitespace)
  corpus = tm_map(corpus, removeNumbers)
  corpus = tm_map(corpus, removePunctuation)
  corpus = tm_map(corpus, content_transformer(removeURL))
  corpus = tm_map(corpus, content_transformer(removeHashtags))
  corpus = tm_map(corpus, content_transformer(removeTwitterHandles))
  corpus = tm_map(corpus, removeWords, stopwords("english"))
  # Stemming - Convert words to their stems, for standardization
  corpus = tm_map(corpus, stemDocument)
  # Generate Document-Term Matrix from matrix, using words of length >= 3
  dtm = DocumentTermMatrix(corpus)
  sentiment = analyzeSentiment(dtm, language = "english",
                               control = list(wordsLengths = c(3, Inf))) 
  sentiment
}


# Split our tweets up for analysis, due to memory restraints
n <- length(rownames(tweets))
partial.1 <- tweets[1:25000,]
partial.2 <- tweets[25001:50000,]
partial.3 <- tweets[50001:75000,]
partial.4 <- tweets[75001:100000,]
partial.5 <- tweets[100001:n,]
split.dfs <- list(partial.1, partial.2, partial.3, partial.4, partial.5)
# Call the getSentiment() function on each of our smaller datasets
# and bind the results into a single file for all of the Tweets
for (i in 1:5) {
  if (i == 1) {
    partial.sentiment <- getSentiment(split.dfs[[i]])
  } else {
    partial.sentiment <- rbind(partial.sentiment, getSentiment(split.dfs[[i]]))
  }
}

# Combine the sentiment data back with the Tweet data from the API
tweets <- cbind(tweets, partial.sentiment)

```

Now that we have calculated a sentiment score for each Tweet, let's take a quick look at a sample of Tweets and their scores so that we can get a better idea of what those scores actually mean. Below are 5 Tweets that had fairly positive sentiment scores (>0.3) and 5 Tweets that had fairly negative sentiment scores(<-0.3). We'll be using the sentiment scores calculated using teh QDAP dictionary, as accessed by the SentimentAnalysis R package.

```{r}
set.seed(42)
positive <- subset(tweets, (tweets$SentimentQDAP > 0.3 & tweets$Retweet != 'Retweet'))
negative <- subset(tweets, (tweets$SentimentQDAP < -0.3 & tweets$Retweet != 'Retweet'))
pos.sample <- sample_n(positive, 5)
neg.sample <- sample_n(negative, size=5)
for (i in 1:5) {
  message(paste0("Score: ", paste( pos.sample[i,'SentimentQDAP'],
                                 pos.sample[i,'Text'], sep=":")))
}

for (i in 1:5) {
  message(paste0("Score: ", paste( neg.sample[i,'SentimentQDAP'],
                                 neg.sample[i,'Text'], sep=":")))
}

```

While our sentiment scores seem to be doing okay, there are some obvious problems, which are generally present in most sentimental analyses based on a Bag of Words approach. Namely, that context can change the meaning of words that might usually be considered positive/negative. Even though we need to be a little wary about scores on specific Tweets, let's go ahead and take a look at our sentiment scores overall, to see if there are any noticeable trends.

```{r}
# Categorize Tweets as either having "#DisneyPlus#, "#DisneyPlusFail", or both of them
tweets$Descriptor <- ifelse((tweets$DisneyPlus == 1 & tweets$DisneyPlusFail == 0), "#DisneyPlus",
                       ifelse((tweets$DisneyPlus == 0 & tweets$DisneyPlusFail == 1),
                              "#DisneyPlusFail","Both"))

# Create boxplots for our 3 categories of Tweets, and their respective sentiments
p1 <- ggplot(tweets, aes(x=Descriptor, y=SentimentQDAP, color=Descriptor)) +
  geom_boxplot() +
  theme_minimal() +
  labs(x="Hashtag used", y="Sentiment Score") +
  theme(legend.position="none") +
  ggtitle("Sentiment by Hashtag")
p2 <- ggplot(tweets, aes(y=SentimentQDAP)) +
  geom_boxplot() +
  theme_minimal() +
  ggtitle("Overall Sentiment")
grid.arrange(p1,p2, ncol=2)
```

While it looks like there is a pretty wide variety of opinions about DisneyPlus, and we have to be cautious about trusting our sentiment scores too much, there does seem to be a bit of a trend here. People using the #DisneyPlusFail hashtag appear to have a generally more negative view of DisneyPlus, which me might expect, based on the name of the hashtag. To put numbers on it, **only ~25% of Tweets using only #DisneyPlus were negative, while at least 50% of Tweets using only #DisneyPlusFail were negative**. That suggests that while our sentiment scoring system is imperfect on specific Tweets, it probably isn't drastically incorrect overall. **Fortunately for Disney, at least 75% of overall Tweets were neutral or positive.**

### Keyword Identification Using N-Grams

Based on the above, it would be useful to know what the happy customers are so happy about, and what the negative Tweets are complaining about, so that Disney can better respond to their customer feedback. 

Let's begin with the negative feedback. Here, we take a subset of negative Tweets and perform an n-gram analysis to find **phrases most commonly found in negative Tweets**, using the ngram R package, and repeat the cleaning steps that were performed above during the sentiment analysis. The cleaning steps will remove common words (like "the", "is", "and"), and will cut words down to their 'stem' word, so our output might not grammatically make sense, but it should give us a good idea about the general topics being discussed in these negative Tweets. In addition to subsetting our dataset to only include Tweets with a negative score, let's filter out Tweets that are obviously advertisements. As a personal choice, I'm going to filter out Retweets as well, since the inclusion of multiple Tweets with identical text may introduce a bias to our analysis, since any word or phrase in those Tweets will be over-represented in our dataset and wouldn't necessarily be representative of a random sample. We'll first setup a helper function for text cleaning and word extraction, then we'll move on to the analysis.

Helper function:

```{r, warning=FALSE}
# Helper function
clean <- function(tweet.subset) {
  # Subset so that we are looking at original Tweets only, rather than including Retweets
  # Also turns our there are some accounts spamming advertisements for other products,
  # using the #DisneyPlus hashtag for publicity. Remove those Tweets + all Retweets,
  # since having the exact same Tweets repeatedly will bias our word counts.
  tweet.subset <- subset(tweet.subset, tweet.subset$Retweet == "Original Tweet")
  tweet.subset <- tweet.subset[!grepl("fortnite|minecraft", tweet.subset$Text),]
  tweet.subset <- unique(tweet.subset[,'Text'])
  # Merge the negative Tweets into a single character vector
  negative.text <- concatenate(tweet.subset)
  #text <- concatenate(tweets[,"Text"])
  # Perform some cleaning of the text, similar to what was
  # done during the sentiment analysis.
  corpus = SimpleCorpus(VectorSource(negative.text))
  corpus = tm_map(corpus, stripWhitespace)
  corpus = tm_map(corpus, removeNumbers)
  corpus = tm_map(corpus, content_transformer(tolower))
  corpus = tm_map(corpus, content_transformer(removeURL))
  corpus = tm_map(corpus, content_transformer(removeHashtags))
  corpus = tm_map(corpus, content_transformer(removeTwitterHandles))
  corpus = tm_map(corpus, removeWords, stopwords("english")) 
  corpus = tm_map(corpus, removePunctuation)
  corpus = tm_map(corpus, stemDocument)
  corpus
}
```

Negative Tweet n-gram analysis:
```{r, warning=FALSE}
# Subset our Tweets to only have the ones with negative sentiment scores
negative.tweets <- subset(tweets, tweets$SentimentQDAP < 0)
# Clean and tokenize Tweets
corpus.neg <- clean(negative.tweets)
# Convert back from corpus object to character vector
negative.text <- concatenate(lapply(corpus.neg, "[", 1))
neg.ng <- ngram(negative.text, n=3)
get.phrasetable(neg.ng)
```

As we might have guessed based on the press release from Disney, 3 of the 5 most common phrases in Tweets with negative sentiment have to do with crashes in the service on launch day. Interestingly, the most common phrase actually is unrelated to the crash in services, and is actually related to the 'outdate cultural depictions', probably referring to the older movies that are available on Disney+. 

Let's repeat the same process using Tweets with a positive sentiment, to see if we can find anything that people particularly seem to like about Disney+. 

```{r, warning=FALSE}
# Subset our Tweets to only have the ones with positive sentiment scores
positive.tweets <- subset(tweets, tweets$SentimentQDAP > 0)
corpus.pos <- clean(positive.tweets)
# Convert back from corpus object to character vector
positive.text <- concatenate(lapply(corpus.pos, "[", 1))
pos.ng <- ngram(positive.text, n=3)
get.phrasetable(pos.ng)
```

The first thing we notice is that many of the most **frequent phrases in positive Tweets have to do with different shows or content**, such as Disney Channel Original Movies, The World According to Jeff Goldblum, the High School Musical series, and the new Star Wars series on Disney+. People also seem to frequently be asking what the first thing was that people are watching. You'll also note, if you click the 'Next' button above a couple times, that more **nostalgic topics are quite prevalent** as well, including the trimmed phrase "feel like kid". It's worth noting that changing the length of n-gram used, you find new titles of interest (for example, series/movies with longer titles appear more often, since they are a long series of words that almost always appear with each other). 

One noteworthy thing here, and a good example of how sentiment analysis has it's pitfalls, is that at least 2 of the top 10 phrases here are about having a "**continue watching section**" or a "**continue watching feature**". These Tweets were marked as having positive sentiment, but let's take a closer look at some of the Tweets including that phrase:

```{r}
set.seed(42)
continue.watch <- tweets[which(grepl("continue watching", tweets$Text)),]
examples <- sample_n(continue.watch, 5)
message(paste0("Number of Tweets with the phrase 'continue watching': ", length(rownames((continue.watch)))))
for (i in 1:5) {
  message(paste0("Score: ", paste(examples[i,'SentimentQDAP'],
                                examples[i,'Text'], sep=":")))
}
```

Based on the above, it looks like **there were `r length(rownames(continue.watch))` Tweets talking about how Disney+ needs to implement some sort of "Continue Watching" feature**, and that most of them were assigned a positive sentiment score. In fact,  only `r sum(continue.watch$SentimentQDAP < 0)` out of the `r length(rownames(continue.watch))` Tweets about this topic had a negative sentiment score. This is a good example of how sentiment analysis can be informative, but at times misleading, depending on the methods used.

Knowing that there are potential pitfalls in our sentiment scores, and just to see if there is anything else that we have missed, let's look at what phrases were most common in the neutral Tweets. 

```{r, warning=FALSE}
# Subset our Tweets to only have the ones with neutral sentiment scores
neutral.tweets <- subset(tweets, tweets$SentimentQDAP == 0)
corpus.neutral <- clean(neutral.tweets)
# Convert back from corpus object to character vector
neutral.text <- concatenate(lapply(corpus.neutral, "[", 1))
neutral.ng <- ngram(neutral.text, n=3)
get.phrasetable(neutral.ng)
```

The top ten phrases from the neutral results look almost exactly the same as many of the top 10 results for the positive sentiment Tweets. What may be of concern is that if you click to the second page of the list, 4 of the phrases ranked there involve people **Tweeting about their desire to cancel their Disney+ subscription!** These are clearly Tweets with a negative view of Disney+, but they were ranked as neutral. Disney should work to address the negative feedback that we've identified if they want to reduce the number of people that are apparently proud to say that they decided to cancel their subscription only a couple days after the streaming service opening day.

Of course, any analysis like this would be incomplete without doing a wordcloud, just for fun. Let's throw together a quick one, using just a sample of 20,000 Tweets, as we wrap up our analysis.

```{r}
tweet.subset <- sample_n(tweets, 20000)
tweet.subset <- tweet.subset[!grepl("fortnite|minecraft", tweet.subset$Text),]
tweet.subset <- unique(tweet.subset[,'Text'])
negative.text <- concatenate(tweet.subset)
corpus = SimpleCorpus(VectorSource(negative.text))
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, content_transformer(removeURL))
corpus = tm_map(corpus, content_transformer(removeHashtags))
corpus = tm_map(corpus, content_transformer(removeTwitterHandles))
corpus = tm_map(corpus, removeWords, stopwords("english")) 
corpus = tm_map(corpus, removePunctuation)
dtm <- TermDocumentMatrix(corpus)
mat <- as.matrix(dtm)
vec <- sort(rowSums(m), decreasing=TRUE)
df <- data.frame(word=names(v), freq=v)
set.seed(42)
wordcloud(words = df$word, freq= d$freq, min.freq = 5,
          max.words = 200, random.order=FALSE, rot.per=0.1,
          colors=brewer.pal(8, "Paired"))
```


## Conclusion
In the end, Disney+ looks like it has had a pretty successful launch week in the United States, according to Twitter. Almost anytime a new product results in over 100,000 Tweets within a couple days of being released, it's a good thing, but let's review a few of the more specific findings that our analysis revealed:

### Praise for Disney+
* Fans were pleased both by **new content** like the Star Wars series, as well as more **nostalgic classics**
  + In addition to addressing the negative feedback, providing more content (especially nostalgic content) should help increase customer satisfaction
* **Most Tweets were neutral or positive** in nature, though there are limitations with our sentiment scores

### Critical Feedback
* Most negative Tweets had to do with **frustration with the server crashes** that happened shortly after the release
* Many people want Disney to implement some sort of **continue watching feature**
* Some users seem to have been upset by **outdated cultural depictions** in older movies





```{r include=FALSE}
#Saving this for convenience. Not part of story.
#saveRDS(file="tweets_with_sentiment.rds", tweets)
#saveRDS(file="neg_ng.rds", neg.ng)
#saveRDS(file="pos_ng.rds", pos.ng)
#saveRDS(file="neutral_ng.rds", neutral.ng)
tweets <- readRDS("tweets_with_sentiment.rds")
neg.ng <- readRDS("neg_ng.rds")
```























